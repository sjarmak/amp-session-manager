# benchmarks/model-benchmark-v2.yaml
version: 2
name: "Amp Session Orchestrator – Model Benchmark v2"
description: >
  End-to-end evaluation covering smoke tasks, interactive repo work,
  and SWE-bench bug-fixing across default, gpt-5 and alloy modes.
  Now unified with QA evaluation capabilities.

###############################################################################
# GLOBAL DEFAULTS – may be overridden per-suite or per-case
###############################################################################
defaults:
  base_branch: main
  parallel: 4               # how many repos to run in parallel
  max_iterations: 10
  timeout_sec: 900          # 15 min wall-clock per case
  json_logs: true           # force --stream-json for telemetry extraction
  merge_on_pass: false      # never merge – we only measure

###############################################################################
# MODELS / RUN CONFIGS
###############################################################################
models:
  default:
    name: default           # no CLI overrides → whatever amp chooses
  gpt5:
    name: gpt-5
    amp_args: ["--try-gpt5"]
  alloy:
    name: alloy
    amp_args: ["--blend", "alloy-random"]

###############################################################################
# METRICS – the report generator will aggregate these for each model
###############################################################################
metrics:
  - success_rate            # passed / total
  - avg_iterations          # average iterations per case
  - total_runtime_sec       # total wall clock time
  - total_tokens            # total token usage across all cases
  - pass_rate               # for QA cases

###############################################################################
# EVALUATION SUITES
###############################################################################
suites:

  # 1. QA evaluation suite - fast question/answer tests
  - id: qa_smoke
    description: "Quick QA tests to validate basic functionality"
    cases:
      - id: basic_math
        kind: qa
        eval_spec: "evals/basic-math.yaml"
        timeout_sec: 60
      - id: code_generation
        kind: qa  
        eval_spec: "evals/code-gen.yaml"
        timeout_sec: 120

  # 2. simple-smoke – should finish in one pass, good sanity check
  - id: smoke
    description: "Single-turn CRUD / scaffolding tasks – validates harness."
    cases:
      - id: hello_readme
        kind: session
        repo: /tmp/test-repos/Hello-World
        prompt: |
          Create a README.md that explains how to run the project, add it,
          commit it, and make sure `cat README.md` shows the expected text.
        script_command: "grep -q 'Hello World' README.md"
        
      - id: add_function
        kind: session
        repo: /tmp/test-repos/simple-math-lib
        setup_script: |
          echo 'def sub(a,b): return a-b' > math.py
        prompt: |
          Add a function `add(a: int, b: int) -> int` to math.py and a pytest
          file tests/test_add.py that asserts add(2,3)==5.
        script_command: "pytest -q"

  # 3. interactive-repo – deliberately needs ≥3 iterations to trigger
  #    model swaps in alloy mode.
  - id: interactive_repo
    description: "Multi-turn iterative tasks (refactor, optimise, test)."
    cases:
      - id: refactor_requests_json
        kind: session
        repo: /tmp/test-repos/requests
        prompt: |
          Refactor requests.models.Response.json() to use the std-lib json
          module instead of simplejson while keeping identical behaviour.
        follow_up_prompts:
          - "Now optimise for performance when content length > 1 MiB."
          - "Add type hints and update/extend the unit tests."
        script_command: "pytest tests/test_json.py -q"
        
      - id: typing_fastapi_example  
        kind: session
        repo: /tmp/test-repos/fastapi
        prompt: |
          Introduce Pydantic v2 type-validation in docs/src/tutorial/*
          examples and adjust the corresponding tests.
        follow_up_prompts:
          - "Ensure mypy passes with --strict."
          - "Benchmark performance and summarise the impact."
        script_command: |
          bash -c "pytest -q && mypy docs/src/tutorial --strict"

  # 4. SWE-bench – quick cases that often succeed in ≤2 iterations
  - id: swebench_quick
    description: "30 easy SWE-bench cases (sanity, comparer)."
    swebench_cases_dir: "eval_data/swebench/quick"   # 30 *.json files
    max_iterations: 6

  # 5. SWE-bench – hard cases known to require multiple attempts  
  - id: swebench_hard
    description: "20 challenging SWE-bench cases (multi-turn expected)."
    swebench_cases_dir: "eval_data/swebench/hard"    # 20 *.json files
    max_iterations: 10
