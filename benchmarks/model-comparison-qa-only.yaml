version: 2
name: "Model Comparison: QA Evaluations Only"
description: >
  Quick model comparison using only Q&A evaluations.
  No repositories needed - runs immediately!

###############################################################################
# GLOBAL SETTINGS  
###############################################################################
defaults:
  parallel: 3               # Can run all models at once for QA
  timeout_sec: 120          # 2 min per QA case
  json_logs: true
  merge_on_pass: false

###############################################################################
# MODEL CONFIGURATIONS
###############################################################################
models:
  default:
    name: "Amp Default"
    
  glm45:
    name: "GLM-4.5"
    amp_args: ["--model", "glm-4-plus"]  # Adjust based on actual CLI flags
    
  gpt5:
    name: "GPT-5"
    amp_args: ["--try-gpt5"]

###############################################################################
# METRICS TO TRACK
###############################################################################
metrics:
  - success_rate
  - pass_rate  
  - avg_latency_ms
  - total_runtime_sec

###############################################################################
# EVALUATION SUITES - QA ONLY
###############################################################################
suites:
  - id: reasoning_qa
    description: "Mathematical and logical reasoning tests"
    cases:
      - id: math_reasoning
        kind: qa
        eval_spec: "evals/math-reasoning.yaml"
        timeout_sec: 60
        
      - id: code_understanding
        kind: qa  
        eval_spec: "evals/code-comprehension.yaml"
        timeout_sec: 90
